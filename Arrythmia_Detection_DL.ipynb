{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNm+zIee3RtBMtTA0n5rVan",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NainaMKumar/Arrythmia_detection/blob/main/Arrythmia_Detection_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wfdb"
      ],
      "metadata": {
        "id": "tC0bhl9kqG8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYikiv70nrua"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import scipy\n",
        "import wfdb\n",
        "import os\n",
        "from pathlib import Path\n",
        "from scipy import signal\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from keras import regularizers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam\n",
        "from keras.metrics import Precision\n",
        "from keras.metrics import Recall\n",
        "from keras.metrics import AUC"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_directory = Path('/content/mit-bih-arrhythmia-database-1.0.0.zip')"
      ],
      "metadata": {
        "id": "3RK-umKkn-NL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "with ZipFile(dataset_directory, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done')"
      ],
      "metadata": {
        "id": "zGOrp4bQ_Roj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_directory = Path('/content/mit-bih-arrhythmia-database-1.0.0')"
      ],
      "metadata": {
        "id": "uPnWclpAAouE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Demo 1 - Read a WFDB record using the 'rdrecord' function into a wfdb.Record object.\n",
        "# Plot the signals, and show the data.\n",
        "record = wfdb.rdrecord(dataset_directory / '100')\n",
        "wfdb.plot_wfdb(record=record, title='Record 100 from MIT BIH Arrythmia Database')\n",
        "display(record.__dict__)"
      ],
      "metadata": {
        "id": "Wu0w2-TDoDow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Demo 2 - Read certain channels and sections of the WFDB record using the simplified 'rdsamp' function\n",
        "# which returns a numpy array and a dictionary. Show the data.\n",
        "signals, fields = wfdb.rdsamp(dataset_directory / '100', channels=[0, 1], sampfrom=100, sampto=15000)\n",
        "display(signals)\n",
        "display(fields)"
      ],
      "metadata": {
        "id": "oytnlIqCoFwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract signal data\n",
        "signal_1 = signals[:, 0]\n",
        "signal_2 = signals[:, 1]\n",
        "\n",
        "# Extract sampling frequency and number of samples\n",
        "fs = fields['fs']\n",
        "n_samples = fields['sig_len']\n",
        "\n",
        "# Calculate time array\n",
        "time = np.arange(n_samples) / fs\n",
        "\n",
        "# Create plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(time, signal_1, label='Signal 1')\n",
        "plt.plot(time, signal_2, label='Signal 2')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.title('Signal Plot')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-y_K_2z3oHwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ecg_signals = []\n",
        "\n",
        "for file in sorted(dataset_directory.glob('*.dat')):\n",
        "    print(file.stem)\n",
        "    signals, fields = wfdb.rdsamp(dataset_directory / file.stem, channels=[1], sampfrom=100, sampto=15000)\n",
        "    print(signals)\n",
        "    ecg_signals.append(signals)\n",
        "\n",
        "ecg_signals = np.array(ecg_signals)\n",
        "print(ecg_signals.shape)"
      ],
      "metadata": {
        "id": "VIuPbQhWoIW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_signal(ecg_signal):\n",
        "\n",
        "    # Extract sampling frequency and number of samples\n",
        "    fs = fields['fs']\n",
        "    signal_length = len(ecg_signal)\n",
        "\n",
        "    # Calculate time array\n",
        "    time = np.arange(0, signal_length) / fs\n",
        "\n",
        "    # Create plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(time, ecg_signal)\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Amplitude')\n",
        "\n",
        "    plt.title('Signal Plot')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "SEJmcIPwoKZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scale_signal(ecg_signal):\n",
        "    # Find the minimum value in the signal\n",
        "    min_val = np.min(ecg_signal)\n",
        "\n",
        "    # Calculate the scaling factor to shift the signal above 0\n",
        "    scaling_factor = abs(min_val) + 1  # Add 1 for a small margin\n",
        "\n",
        "    # Scale the signal by adding the scaling factor\n",
        "    scaled_signal = ecg_signal + scaling_factor\n",
        "\n",
        "    return scaled_signal"
      ],
      "metadata": {
        "id": "LFhQZXDToKPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ecg_signal = ecg_signals[0]\n",
        "ecg_signal = np.array(ecg_signal)\n",
        "\n",
        "scaled_ecg_signal = scale_signal(ecg_signal)\n",
        "scaled_ecg_signal = np.array(scaled_ecg_signal)"
      ],
      "metadata": {
        "id": "FQhXt5JDoOQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def signal_filter_powerline(signal, sampling_rate, powerline=50):\n",
        "    \"\"\"Filter out 50 Hz powerline noise by smoothing the signal with a moving average kernel with the width of one\n",
        "    period of 50Hz.\"\"\"\n",
        "\n",
        "    if sampling_rate >= 100:\n",
        "        b = np.ones(int(sampling_rate / powerline))\n",
        "    else:\n",
        "        b = np.ones(2)\n",
        "    a = [len(b)]\n",
        "    y = scipy.signal.filtfilt(b, a, signal, axis = 0)\n",
        "    return y"
      ],
      "metadata": {
        "id": "WjMckUgUoQve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_signal(ecg_signal)"
      ],
      "metadata": {
        "id": "hs8Q_BT2oTkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "denoised_signal = signal_filter_powerline(ecg_signal, fields['fs'])\n",
        "plot_signal(denoised_signal)"
      ],
      "metadata": {
        "id": "YH-FG5RjoV7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_baseline_wander(ecg_signal, sampling_frequency, cutoff_frequency = 1.2, filter_order=3):\n",
        "    nyquist_frequency = 0.5 * sampling_frequency\n",
        "    normalized_cutoff = cutoff_frequency / nyquist_frequency\n",
        "\n",
        "    # Design a high-pass Butterworth filter\n",
        "    b, a = signal.butter(filter_order, normalized_cutoff, btype='high', analog=False)\n",
        "\n",
        "    # Apply the high-pass filter to remove baseline wander\n",
        "    baseline_removed_signal = signal.filtfilt(b, a, ecg_signal, axis = 0)\n",
        "\n",
        "    return baseline_removed_signal"
      ],
      "metadata": {
        "id": "gx1WoNA2oX5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_removed_signal = remove_baseline_wander(denoised_signal, fields['fs'])\n",
        "\n",
        "plot_signal(ecg_signal)\n",
        "plot_signal(baseline_removed_signal)"
      ],
      "metadata": {
        "id": "SncfopK7oZtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_signal(signal):\n",
        "    norm_signal = (signal - np.min(signal)) / (np.max(signal) - np.min(signal))\n",
        "    return norm_signal\n",
        "    # scaler = MinMaxScaler()\n",
        "    # signal = signal.reshape(-1, 1)\n",
        "    # scaled_signal = scaler.fit_transform(signal)\n",
        "    # return scaled_signal"
      ],
      "metadata": {
        "id": "ZQKnQJDtobYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_signal = normalize_signal(baseline_removed_signal)\n",
        "plot_signal(normalized_signal)\n",
        "\n",
        "print(np.min(baseline_removed_signal))\n",
        "print(np.max(baseline_removed_signal))\n",
        "print(np.min(normalized_signal))\n",
        "print(np.max(normalized_signal))\n"
      ],
      "metadata": {
        "id": "2XtfWUvQodNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "\n",
        "for ecg_signal in ecg_signals:\n",
        "    denoised_signal = signal_filter_powerline(ecg_signal, fields['fs'])\n",
        "    filtered_signal = remove_baseline_wander(denoised_signal, fields['fs'])\n",
        "    normalized_signal = tf.keras.utils.normalize(np.array(filtered_signal))\n",
        "    X.append(normalized_signal)\n",
        "\n",
        "X = np.array(X)\n",
        "# print(X)"
      ],
      "metadata": {
        "id": "cnWkjQ7Doe3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = []\n",
        "\n",
        "for file in dataset_directory.glob('*.atr'):\n",
        "    print(file.stem)\n",
        "    annotations = wfdb.rdann(str(dataset_directory / file.stem), 'atr', sampto=15000)\n",
        "    diagnosis = annotations.aux_note\n",
        "    y.append(diagnosis)\n",
        "\n",
        "print(y)\n",
        "print(len(y))"
      ],
      "metadata": {
        "id": "0Vjfiw7Jog4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = [[item.strip().replace('\\x00', '') for item in inner_list if item.strip()] for inner_list in y]\n",
        "\n",
        "y = [[item.replace('(', '') for item in inner_list] for inner_list in y]\n",
        "print(y)\n",
        "print(len(y))"
      ],
      "metadata": {
        "id": "tEfs1syloigU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_dict = {'N': 0, 'P': 1, 'B': 2, 'AFIB': 3, 'MISSB': 4, 'PREX': 5, 'VT': 6, 'VFL': 7, 'SBR': 8}\n",
        "y = [[y_dict.get(k) for k in lst] for lst in y]\n",
        "print(y)"
      ],
      "metadata": {
        "id": "1rcNJUGDoke6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlb = MultiLabelBinarizer()\n",
        "y_transformed = mlb.fit_transform(y)\n",
        "print(y_transformed)\n",
        "y_transformed= np.array(y_transformed)\n",
        "\n",
        "num_classes = mlb.classes_"
      ],
      "metadata": {
        "id": "GUPf_IOYomW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y_transformed, test_size = 0.3, random_state = 42)"
      ],
      "metadata": {
        "id": "W709MnaZBA7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data augmentation\n",
        "noise_mean = 0\n",
        "noise_std = 0.1\n",
        "\n",
        "\n",
        "X_train_reshaped = np.expand_dims(X_train, axis=-1)\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    preprocessing_function=lambda x: x + np.random.normal(loc=noise_mean, scale=noise_std, size=x.shape)\n",
        ")\n",
        "\n",
        "generator = datagen.flow(X_train_reshaped, y_train, batch_size = 500, shuffle=True)\n",
        "augmented_samples = next(generator)\n",
        "\n",
        "augmented_signals = augmented_samples[0]\n",
        "augmented_labels = augmented_samples[1]\n",
        "\n",
        "# print(augmented_signals.shape)\n",
        "augmented_signals = np.squeeze(augmented_signals, axis = (3))\n",
        "\n",
        "X_train = np.concatenate((X_train, augmented_signals), axis=0)\n",
        "y_train = np.concatenate((y_train, augmented_labels), axis=0)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "4bBq1HUiO1Ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train.sum(axis = 0))"
      ],
      "metadata": {
        "id": "FF1H2pau_YqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "bZjlvAYWon-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compute class weights\n",
        "\n",
        "reverse_dict = {v: k for k, v in y_dict.items()}\n",
        "class_frequencies = np.sum(y_transformed, axis=0)\n",
        "print(class_frequencies)\n",
        "num_classes = len(y_dict)\n",
        "\n",
        "class_weights = {reverse_dict[i]: len(y_transformed) / (num_classes * class_frequencies[i]) for i in range(num_classes)}\n",
        "class_weights = {y_dict[label]: class_weights[label] for label in class_weights}\n",
        "print(class_weights)"
      ],
      "metadata": {
        "id": "oWyzg4Lnopj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential()\n",
        "model.add(layers.Conv1D(filters = 32, kernel_size = 5, activation = 'relu'))\n",
        "model.add(layers.MaxPooling1D(pool_size = 2))\n",
        "model.add(layers.Conv1D(filters = 32, kernel_size = 5, activation = 'relu'))\n",
        "model.add(layers.MaxPooling1D(pool_size = 2))\n",
        "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
        "model.add(layers.Dense(100, activation = 'relu', kernel_regularizer = regularizers.l2(0.0003)))\n",
        "model.add(layers.Dense(num_classes, activation = 'softmax', kernel_regularizer = regularizers.l2(0.0003)))\n"
      ],
      "metadata": {
        "id": "60MWFrM92Xuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.0001\n",
        "optimizer = Adam(learning_rate=learning_rate)"
      ],
      "metadata": {
        "id": "VPYmDs6DotTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "model.compile(\n",
        "    loss= 'categorical_crossentropy',\n",
        "    optimizer=optimizer,\n",
        "    metrics = ['accuracy', Precision(), Recall(), AUC(curve = 'ROC')]\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs= 16,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    validation_data = (X_test, y_test),\n",
        "    class_weight = class_weights,\n",
        ")"
      ],
      "metadata": {
        "id": "OtVJpRjoouyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "id": "uE7Tzn6u91RN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(y_pred, axis = 1)\n",
        "print(y_test)"
      ],
      "metadata": {
        "id": "qQa4kQDa-PuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Wz_7msopUgXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['auc'])\n",
        "plt.plot(history.history['val_auc'])\n",
        "plt.title('model auc')\n",
        "plt.ylabel('auc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cXWm8BNXUg-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy, precision, recall, auc = model.evaluate(X_test, y_test, batch_size = 64)"
      ],
      "metadata": {
        "id": "Mj8nWTRQUjKI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}